---
title: "Final Project"
author: "Ansh Gupta"
date: "August 11, 2022"
output: pdf_document
---

Establishing reliable biomarkers for assessing and validating clinical diagnosis at early stages of Parkinson's disease is crucial for developing therapies to slow or halt disease progression. This data set uses whole blood gene expression profiling from over 500 individuals where we will attempt to find a gene signature. This repository contains the gene expression profiles collected in the GENEPARK consortium. The main study sought a classifier for IPD. These data contain 233 healthy controls, 205 IPD patients, and 48 patients with other neurodegenerative diseases (NDD). Other samples are available in the data and can be used for additional analyses. The largest class of these additional samples are 22 samples from genetic unaffected controls and 41 genetic PD patients.

Note: the original study which uploaded this data to NIH Geo is not yet published.

\section{Data Wrangling}

Let's start by loading in our data sets. Download these from the canvas site, and make a new folder for R bootcamp. We'll switch to this directory here. 

```{r, setup, include=FALSE}
## If you have never created an R Markdown document before, go to
## File -> New File -> R Markdown -> click "Yes"

knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "~/Desktop/Dartmouth/QBS103/Final_Project_Final")

r <- getOption("repos")
r["CRAN"] <- "http://cran.cnr.berkeley.edu/"
options(repos = r)

```

The tinyTex package will allow you to actually knit .pdf documents from RMarkdown:

```{r set wd, message=FALSE, warning=FALSE, results=FALSE, echo=FALSE}
# uncomment these for first runthrough
#install.packages("tinytex")
#tinytex::install_tinytex()  # install TinyTeX
```

Note that we have both a phenotype file, as well as a file which includes the normalized and log transformed expression values. We can use the read.csv function to load in these files. 

```{r load data}
# loading the data 
expr = read.csv("simulatedData.csv")
pheno = read.csv("parkPheno.csv")
```

We should start by summarizing both these files. Try the following functions: head(), and View(). Note that while the dimensions on our phenotype file are reasonable, we have 552 columns in our expression file. Just summarize the first 10 columns of this file. 

```{r check out the data}
# your code here
#commenting these out for ease of viewing in the PDF file
#head(pheno)
#View(pheno)
#head(expr)
#View(expr)
```
Try summarizing the phenotype data:

```{r summarize the data}
# your code here
summary(pheno[,1:10])
```

We make the following observations. 

\begin{enumerate}
\item We have some unnecessary data in this file. We aren't interested in the submission and last update date. We can reduce the dimensions of this file so it handles nicer from now on. 
\item We have a LOT of missing data. You'll learn how to handle this in some of your biostats classes! For now, we'll run what analyses we can given the data we have.
\item Some of our scores have been read in as character values (and they should be numbers). If you investigate this further, you'll find that some values have been recorded as "ND", which we'll assume means "no data". We will need to record these as NA values in R. 
\end{enumerate}

Our next step is to address item one. We will reduce the dimensions of our pheno data frame to include only that information that we're interested in modelling. We can exclude the dates, type (as it's all RNA), tissue (all whole blood), organism (all homo sapiens), and subject ID (we will be using geo_accession as our unique indicator). As well, we will exclude mutated_pd_genes, as we indend to define our own gene signature later this week.

Subset your pheno data frame to include columns 1,8,9,11:20.

```{r reduce pheno} 
# your code here
pheno=pheno[,c(1,8,9,11:20)]
head(pheno)
```

Next we need to correct the columns which contain "ND". You can use the "which" function to find the index of of the matrices which are "ND", and then set these to NA. Set columns 8,9,11,12,13 to numeric values using the "as.numeric" function inside a "sapply" loop. Run a summary of the data frame again.

```{r replace ND, warning=F}
#your code here
ix=which(pheno==" ND",arr.ind = T) #storing the indices
pheno[ix] = NA #changing "ND" to NA
j=c(8,9,11,12,13) #storing column numbers 
pheno[,j] = sapply(unlist(pheno[,j]),as.numeric) #changing those column numbers to have numeric vals
summary(pheno)
```

We have a LOT of missing values present in the data! As mentioned before, imputation of missing values is an entire field unto itself. While we won't be imputing data today, we are going to wrangle the above data to attempt to ameliorate some of these missing values. 

To do this we will:

1. Combine our Age variables to be age_at_exam where known, but age_at_symptoms where that is observed without age at exam
2. Combine our updrs scores to be the average updrs
3. Combine our hoehn scores to be the average hoehn
4. Keep our moca score as is
5. Remove the old variables from our pheno dataset. 
```{r}
#part 1 - combining Age variables to AgeMaster col
pheno$AgeMaster = pheno$age_at_exam
pheno$AgeMaster[is.na(pheno$AgeMaster)] = pheno$age_at_symptoms[is.na(pheno$AgeMaster)]

#part 2 - combining updrs scores to overallUPDRS col
up = c(6:10) 
pheno$overallUPDRS = rowMeans(pheno[,up])

#part 3 - combining Hoehn scores to overallHoehn col
hn = c(11,12)
pheno$overallHoehn = rowMeans(pheno[,hn])

#Part 4 & 5 - keeping moca score and removing old variables from pheno dataset
keep = c(1,2,3,13,14,15,16)
pheno = pheno[,keep]

#View(pheno)

```

As you can see, we have far fewer missing values to contend with!

Let's look at a summary of the first 10 columns of expression data set.

```{r sum expr}
# your code here
summary(expr[,1:10])
```

We don't need the X1 variable - this is just remaining row labels in the csv file. Let's remove this variable. 

```{r expr clean}
#your code here
expr$X=NULL
```

We don't see any evidence of missing values in our summary, but we should check all of the columns (excluding the GeneName). You can check this with the "anyNA"" function. 

```{r check NA}
# your code here
which(is.na(expr),arr.ind=T)
```

Let's identify how big this problem is, and where it occurs.

```{r find NA}
# your code here
expr=expr[-nrow(expr),]
```

So one of our gene names is NA! This isn't useful, so let's remove this row.

```{r remove NA}
# your code here
# did this in the previous code chunk!
```

We should see if the unique identifiers in our two data sets match. Check for a perfect match using the "identical" function.

```{r identifier match}
# your code here
identical(colnames(expr[,-1]),as.character(pheno[,1]))
```

So that we don't lose any work, let's clean up our workspace to include only our cleaned expression and pheno data sets, which we can reload later. 

```{r save workspace, echo=F}
rm(j)
rm(ix)
save.image(file="RbootcampDay1.RData")
```


\section{Exploratory Data Analysis}

In this section we are going to explore some of the data we have, and maybe develop a diagnostic signature for Parkinson's disease.

First, load in your data from yesterday. 

```{r load day 1, echo=F}
load("RbootcampDay1.RData")
```

Let's re-examine our pheno data set with the summary function again. 

```{r pheno sum}
summary(pheno)
```

We need to further delve into our disease label in order to simplify some of this analysis. Attach your pheno data frame using the attach function, and then summarize the disease label vector.

```{r disease lab, warning=F}
attach(pheno)
summary(disease_label)
```

Here we have the counts of all the diseases in our data set. If you look at the actual excel file (not the csv), I've put in a dictionary for these acronyms if you're curious. Here, our controls and our genetic unaffected are both considered to be healthy controls. Any label which contains PD is some subset of Parkison's Disease, and the other labels represent other neurological disorders. We need to make a variable which records a 1 for our cases, and a 0 for our controls. Here, since we are interested in a signature that distinguishes PD from our other disease, the other diseases are technically part of the control set.

Try to set your case control vector using the grep function to find the indicies which contain "PD". At the end, sum your case vector to check that it worked. Make another variable of the words "case" and "control"

```{r case vector}
# your code here
pdI=grep("PD",disease_label,value = F)
case=rep(0,length(disease_label))
case[pdI]=1
sum(case)
##alternative
case=grepl("PD",disease_label)*1
sum(case)
caseName = ifelse(case == 1, "case","control")
```

We need to find differentially expressed genes. You'll learn more about this later. For now, feel free to use some of my code. Start by downloading the limma package

```{r limma, warning=F, message=F, results=F}
## If using Windows, first go to https://cran.rstudio.com/bin/windows/Rtools/ and install the appropriate version of Rtools
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
#BiocManager::install("limma")
library(limma)
```

We will use the following code. 
**Comment this code with your thoughts below.**

```{r find some genes}
#subset our data for a training and test set
set.seed(2) #generating random numbers
prob = runif(ncol(expr)-2) #using a random uniform distribution
k = which(prob>=0.3333333) #selecting indices with probability >= 0.3333333 for training puposes
eset = expr[,2:ncol(expr)] #subsetting all cols except the first col for expr into eset
eset = eset[,k] #keeping indices with prob >= 0.3333333 in the eset subset
rownames(eset) = expr[,1] #setting rownames of the subset same as expr
design = model.matrix(~0+as.factor(case[k])) #creating a design/model matrix
fit = eBayes(lmFit(eset,design)) #using empirical bayesian model for differential expression
topTable(fit, coef=2) #getting a table of the top ranked genes
results = topTable(fit, coef=2, number=Inf) #storing the top ranked genes in results
```
Here, we have our gene names, our log fold change for expression, average expression, t statistic, pvalue, adjusted pvalue (for multiple testing!!), and the log odds of differential expression.

Next, we select those genes that have adjusted p-values below 0.001. **Comment the code with your thoughts about what its doing below.** 

```{r filter some genes}
#using the false discovery rate to get the rate of type 1 errors in the null hypothesis by selecting the genes with adjusted p-values below 0.001 
selected  = row.names(results)[p.adjust(results$P.Value, method="fdr")<0.001] 
direction = sign(results$logFC) #storing the signs as 1,0,-1 for numbers in logFC col
esetSel = eset[selected, ] #subsetting rows with adjusted p-values below 0.001
nrow(esetSel) #checking number of genes in subset
```

Okay! So we're now looking at just 175 genes!

We are going to make a heat map here. I've provided the code, but **try changing colours, labels, etc. to make it your own.**

```{r make heat map, fig.height=10}
library("RColorBrewer")
patientcolors <-ifelse(case[k]==1,"yellow","black") #cases are yellow, and controls are black
col <- colorRampPalette(brewer.pal(27, "Blues"))(256)
heatmap(as.matrix(esetSel), scale = "none", col =  col, ColSideColors = patientcolors, distfun = function(x) dist(x,method = 'canberra'))
```

Notice the annotation bar along the top. This indicates PD vs not PD samples. This heat map is an example of a 'non-supervised method' - where we didn't feed the labelled data to the algorithm. Instead, it is just clustering similar samples together. Because all of our PD samples cluster away from the non-PD samples, we are relatively certian we've picked good biomarkers! We should also check a PCA plot.

```{r make PCA}
pc<-prcomp(t(esetSel),center=T,scale=T)
plot(pc,type="l",main="Checking the number of Principle Components")
```


Again, I've provided code for you here. **Change it to something you like better!**


```{r pca plpt, fig.height=10, warning=FALSE, message=FALSE}
#install.packages("devtools")
library(devtools)
library(ggpubr)
#install_github("vqv/ggbiplot")
library(ggbiplot)
g = ggbiplot(pc, obs.scale = 1, var.scale = 1, 
              groups = as.factor(caseName[k]), ellipse = T, 
              circle = T, labels=disease_label[k],var.axes = F)
g = g + scale_color_discrete(type = getOption("ggplot2.discrete.fill"))
g = g + theme(legend.direction = 'horizontal', plot.title = element_text(hjust = .5, family="sans"), 
        text = element_text(family = "serif"), legend.position = 'left')
g = g + theme_cleveland()
g = g + ggtitle("BiPlot for Case vs Control") 
print(g)
```

We have separation! Notice the obvious differences between cases and controls.

Make a variable which only contains the differential gene names and call it diffGenes AND print out all of these gene names using one line of code.

```{r get gene names}
#your code here
(diffGenes = selected)
```

To use these genes as a classifier, we will need to define a score function. Our score will be the sum of the average expression for the upregulated (positive) genes and the average for the down regulated (negative) genes. Here, I've written you a function which will do this. Please enter it and **make comments to show you understand what its doing.** 

```{r make function}
PDscore<-function(x,g,v,s){
  #x expression values for a sample
  #g all the genes
  #v the diffGenes
  #s is the sign of the logFC
  
  i = which(g%in%v)
  x = x[i] #updating the expression values for a sample
  s = s[i] #updating the sign of logFC values in the sample
  p = c() #initializing vector p
  n = c() #initializing vector n
  
  for(i in 1:length(x)){  #checking sign of logFC scores that are > 0 in the sample
    if(s[i]>0){
      p<-append(p,(x[i])) #appending these values to vector p
    }
    else if(s[i]<0){
      n<-append(n,(x[i])) #appending the logFC scores with negative signs to vector n
    }
  }
  
  #replacing null values with 0 for vectors p and n
  if(is.null(p)){p[1]=0}
  if(is.null(n)){n[1]=0}
  
  score = mean(p)-mean(n) #taking the difference of the means of the positively and negatively signed logFC scores
  return(score) #returning that mean difference
}
```

Now we can apply our function to our expression set to define a score for each patient. **Comment what this is doing and why each step is necessary!**

```{r apply function}
score<-c() #initializing vector score

#searching for the Gene Names in results
allGenes<-as.character(expr[as.character(expr$GeneName)%in%rownames(results),1]) 

#calculating PD Scores for each specific Gene Name

for(i in 1:ncol(eset)){ #updating scores for the entire subset eset
  score[i]<-PDscore(eset[,i],allGenes,diffGenes,direction)
}

hist(score,main="Distribution of our PD Scores") #histogram for our PD Scores
```

Now we'll use ggplot to make and interpret a violin plot of our score. I've provided some code to do this, but **try to change labels, colours, etc. to make it your own.** 

```{r violin plot, fig.height=10, fig.width=7, warning=F, message=F}
#install.packages("ggpubr")
library(ggpubr)

df = data.frame(cbind(case[k],score))

dp = ggplot(df, aes(x=as.factor(case[k]), y=score, fill=as.factor(case[k]))) +
  geom_violin(trim=F)+
  scale_color_brewer(palette="Dark2") + 
  geom_boxplot(width=0.1, fill="black")+
  labs(title="Plot of case by score",x="Case ", y = "Score") +
  stat_compare_means(label.x = 1.5, label.y = 1, size=10)+
  stat_summary(fun.y=median, geom="point", size=5, color="red") + 
  geom_boxplot(width=0.1, outlier.colour="black", outlier.shape=8, outlier.size=4, notch=T) + 
  geom_dotplot(binaxis='y', stackdir='center', dotsize=0.2, fill="#ffff00") + 
  scale_color_grey() +
  stat_compare_means(aes(label = ..p.signif..), label.x = 1.5, label.y = 0.9, size =10)
dp + theme(plot.title = element_text(hjust = .5, family="sans"), text = element_text(size = 18))
```

This shows not only the boxplot of our data, but also the distribution of our data points around the boxplot! As before, we can see that we DO have significant separation for our score, and we can see that the cases are trending to have a higher score. With more time and data cleaning we may be able to find something here!

Let's make a roc plot, first with our training data, and then with our test data. As before, **play with the plot options to make something you like!**

```{r ROC plot 1, warning=F, message=F}
#install.packages("verification")
#install.packages("pROC")
library("pROC")
testEset<-expr[,2:ncol(expr)]
testEset<-testEset[,-k]
newScore<-apply(testEset,2,FUN=PDscore,allGenes,diffGenes,direction)
plot.roc(case[k]~score, data=df,legacy.axes=T,print.auc=T, ci=T, col="darkgreen",main="AUC for Diagnostic Score")
plot.roc(case[-k]~newScore,data=data.frame(cbind(case[-k],newScore)),add=T,print.auc=T, ci=T, col="red", print.auc.y=0.4) 
legend("topleft",c("Training Data","Test Data"),lty=c(1,1),col=c("darkgreen","red"))
```

Notice that our score does better with our training data - this is expected! This is why we need to split our data, to avoid problems with over-fitting. These scores are better than random (the grey line), but we'd like to see an AUC as close to 1 as possible. Let's see if we can do better!

\section{Statistics!}

We can run a t-test to see if our score is significantly different between cases and controls. Try using the t.test function in R.

```{r t test}
# your code here
allScore=c(score, newScore)
mergeCase=c(case[k],case[-k])
t.test(allScore[mergeCase==0],allScore[mergeCase==1])
```

The mean scores for our cases and controls are close, but they are significantly different with an extremely small p-value of 2.787e-13. This highlights a classical statistical fallacy - while small p-values are great, they are often meaningless without a large enough effect size. Here, we have achieved significance due to the large sample size of our study, hence our study is adequately powered. 

We could also run a simple regression to examine the impact of the score on the log odds of being a case. 

```{r small model}
# your code here
smallModel = glm(case[k]~score,family="binomial")
summary(smallModel)
```

Summarize this output!

Again, we conclude that the score is a statistically significant indicator of the odds of having PD. Let's build a larger model which examines other phenotype variables.

First, build a data frame which includes all the model data we're interested in. Start with the age variables in your pheno set, and then use the cbind() function to add on our scores and the binary case vector. Print a summary of the model data.
```{r subset exp}
# your code here
View(pheno)
modelData = cbind(pheno[k,3:7],score)
summary(modelData)
```

We should examine the correlations in our data set. You can do this quickly by building a correlation plot matrix. 

```{r corr plot, fig.height=10, warning=F, message=F}
#install.packages("corrplot")
library(corrplot)
M<-cor(modelData[,-1],use="pairwise.complete.obs") #for  missing data
corrplot.mixed(M)

```

How would you interpret this output? **Answer below!**
We can see that there exists a strong positive correlation between the moca_score and overallHoehn. Moreover, there is also a positive correlation between overallUPDRS and score. All the other features are either mildly positively correlated or not correlated. There does not exist a negative correlation between any of the features. 
For interpretation: blue indicates positive correlation, red indicates negative correlation and white indicates no correlation. Moreover, the size of the circle indicates the positive value of the correlation.

Let's build our first model. Here, we consider the case as our dependent variable, and the others as our explanatory variables. 

```{r first model}
model1<-glm(case[k]~.,family=binomial,data=modelData)
summary(model1)
```

 We will iteratively remove variables with the highest p-values, and then rerun the model until we have our optimal fit!. 

Try this on your own first. 

```{r reduce bayes}
# your code here 
modelData$case = case[k]
modelData2 = na.omit(modelData)
model2 = glm(case~sex+AgeMaster+moca_score+overallUPDRS+overallHoehn+score,family=binomial,data=modelData2)
summary(model2)

modelData2$sex = NULL 
modelData2$AgeMaster = NULL

model3 = glm(case~.,family=binomial,data=modelData2)
summary(model3)
```

This is our final model! Notice that our largest effect size is controlled by our genetic score. At a first glance, we might assume this means that the score has the largest effect on the model. However, if we recall how to interpret our coefficients, the estimated effect size is the change in log odds of being a case for a 1 unit increase in our score. Think about the score distribution: the range of our scores is fairly small. In contrast, the range of the updrs scores varies from 0 to 36. Keep in mind the scale of our data when interpreting these models!

Compare this to your outcome if you use a step function to reduce the model:

```{r}
#your code here
stepModel<-step(model3)
summary(stepModel)
```

Notice that our step-wise reduced model chose to keep both age and overallHoehn despite the insignificant p-value. Why? Answer below!

My step-wise model has successfully removed cols with insignificant p-values, including AgeMaster and overallHoehn. 

Let's predict the probability of having a case given our manually reduced model. Make a histogram of the score from this model. 

```{r predict with model,warning=F}
# your code here
modelScore=predict(stepModel,newdata = modelData)
hist(modelScore, main = "Histogram of Logistic Regression Model")

```

Like before, we'll build a violin plot to compare the output of our regression model. See if you can adapt the violin plot code from before to do this now. 

```{r violin plot2, fig.height=10, fig.width=7}
# your code here
library(ggpubr)

df<-data.frame(cbind(case[k],modelScore))

dp <- ggplot(df, aes(x=as.factor(case[k]), y=modelScore, fill=as.factor(case[k]))) +
  geom_violin(trim=FALSE)+
  geom_boxplot(width=0.1, fill="white")+
  scale_color_brewer(palette="Dark2") + 
  labs(title="Plot of case by score",x="Case ", y = "Score")+
  stat_compare_means(label.x = 1.5, label.y = 1, size=10)+
  geom_dotplot(binaxis='y', stackdir='center', dotsize=0.1, fill="#ffff00") + 
  stat_compare_means(aes(label = ..p.signif..), label.x = 1.5, label.y = 0.9, size =10)
dp + theme(plot.title = element_text(hjust = .5, family="sans"), text = element_text(size = 18))
```

Now we're starting to see a clearer separation of scores! It's clear that by including the established tests to pre-screen patients for PD and other neurological diseases we have improved overall performance. While this may be an obvious conclusion, it is worth noting that the context with which our diagnostic signature would be used would be on patients already exhibiting potential PD symptoms. Clearly this needs a little more work, but for a first pass at assessing raw data, it's not bad!


Again, we can examine ROC curves. I've done some of the set up to get the data in the right format. Use the ROC code above to then build your own plot!

```{r ROC plot 2, warning=F}
library("pROC")
nd = cbind(pheno[-k,],newScore)
colnames(nd) = c(colnames(nd[1:ncol(nd)-1]),"score")
newMScore = predict(stepModel,newdata=nd)

plot.roc(case[k]~score, data=df,legacy.axes=F,print.auc=T, ci=T, main="AUC for Diagnostic Score", col="red")
plot.roc(case[-k]~newScore,data=data.frame(cbind(case[-k],newScore)),add=T,print.auc=T, ci=T, col="darkgreen", print.auc.y=0.4)
legend("topleft",c("Training Data","Test Data"),lty=c(1,1),col=c("red","darkgreen"))

#your code for ROC plots here
```

Here, we have a notable increase in AUC, particularly for our training data. Our test data shows an overal improvement as well, although with a large confidence interval. There are clearly some data points in here which are abnormal - and perhaps worth investigating.

##Congratulations, you have finished the R Bootcamp Assignment!